\section{Methods} 


\begin{figure}[hp]
    \centering
    \includegraphics[width=0.9\linewidth]{Results/pipeline.pdf}
    \caption{Caption}
    \label{fig:placeholder}
\end{figure}

The proposed method aims to model the full correlation structure of a complex system, which may be analytically intractable or require more data than available. To address this, we combine simpler components that can be trained independently and efficiently.

The method takes as input:
an optional set of covariates $\{X_t\}^{T+k}_{t=1}$
$\{X_i\}^{T+k}_{i=1}$ and observations $\{y_i\}^{T}_{i=1}$ a returns a correlation corrected forecast $\{\hat y_i\}^{T+k}_{i=T}$ either as points forecasts, or a number of simulated trajectories.

The overall procedure consists of five steps, summarized below and illustrated in Figure \ref{fig:pipeline}:

\begin{enumerate}
    \item Estimate marginal quantiles at each time point.
    For each $t$, compute
    \[
      \hat{\mathbf{q}}_t
      \;=\;
      f_Q(X_t)
      \;=\;
      \big[q_{\alpha_1}(X_t),\, q_{\alpha_2}(X_t),\, \dots,\, q_{\alpha_m}(X_t)\big]^{\!\top},
    \]
    where $\alpha_i \in (0,1)$ are the quantile levels (e.g., $0.05, 0.10, \dots, 0.95$),
    and $q_{\alpha_i}(X_t)$ denotes the estimated $\alpha_i$-quantile.

    \item  Construct the full marginal distribution from quantiles.
    Using the estimated quantiles, define the predictive CDF
    \[
      \hat{F}_t(x) \;=\; p\!\left(x \,\middle|\, \hat{\mathbf{q}}_t\right).
    \]

    \item Transform observations to pseudo-residual space.
    Apply the probability integral transform followed by the standard normal inverse CDF:
    \[
      z_t \;=\; \Phi^{-1}\!\big(\hat{F}_t(y_t)\big),
    \]
    where $\Phi$ is the CDF of the standard normal distribution.

    \item Predict or simulate in pseudo-residual space.
    Using a correlation model $H$, generate $k$-step-ahead forecasts:
    \[
      \hat{z}_{t+h} \;=\; H(z_t), \qquad h=1,\dots,k.
    \]

    \item Transform forecasts back to the original space.
    Apply the inverse transformations to obtain forecasts on the original scale:
    \[
      \hat{y}_{t+h}
      \;=\;
      \hat{F}_{t+h}^{-1}\!\big(\Phi(\hat{z}_{t+h})\big),
      \qquad h=1,\dots,k.
    \]
\end{enumerate}


\subsection{Marginal Model – Neural Networks}

To estimate marginal quantiles, we employ neural networks (NN) due to their flexibility as universal function approximators. Neural networks can represent highly complex, non-linear relationships without assuming a specific parametric form, making them well-suited for modelling diverse distributions.

Neural networks have become a dominant approach in modern machine learning, and their theoretical foundations are well-documented. For a comprehensive overview, see \citet{schmidhuber2015a} or other standard deep learning resources.

One of the main advantages of neural networks is their model-free flexibility. Inputs can be of arbitrary size or structure enabling broad applicability. Using neural networks for quantile estimation allows the proposed framework to generalize across different domains without altering its overall structure.

However, there are notable challenges. Non-parametric methods like neural networks typically require more data compared to parametric models, and training deep networks can be computationally expensive. Overfitting is also a common issue, particularly in deep architectures. To mitigate this, strategies such as regularization, data augmentation (e.g., adding noise to inputs), and architectural constraints are often employed.

\subsubsection{Architectures \& Implementation}

We implement two architectures:
\begin{enumerate}
    \item \textbf{Simple:} A baseline model performing quantile regression on a linear one-dimensional basis.
    \item \textbf{Latent:} A deep LSTM-based model operating in a reduced latent space.
\end{enumerate}

Both models output 19 quantiles corresponding to levels:
\[
\alpha \in \{0.05, 0.10, \dots, 0.90, 0.95\}.
\]

The motivation for these two models builds on the modular framework introduced by \citet{jorgensenSequentialMethodsError2025} . The latent model aims to learn a compact representation of the input data for downstream tasks, while the simple model serves as a minimal baseline.

\insert{results/models}

% \begin{table}[ht]
% \centering
% \caption{Architecture details. Left: Simple model. Right: Latent LSTM model.}
% \label{tab:arch_side_by_side}
% \begin{minipage}{0.47\linewidth}
% \centering
% \subcaption*{Simple model}
% \begin{tabular}{lll}
% \hline
% Layer & Type  & Activation \\
% \hline
% Output head & Dense ($m$) & Identity \\
% \hline
% \end{tabular}
% \end{minipage}\hfill
% \begin{minipage}{0.47\linewidth}
% \centering
% \subcaption*{Latent model}
% \begin{tabular}{llll}
% \hline
% Layer & Type & Width/Shape & Activation \\
% \hline
% Proj (1) & Dense & $d_\ell$ & SELU \\
% Proj (2) & Dense & $d_\ell$ & SELU \\
% Encoder  & LSTM  & $h$      & -- \\
% Head (1) & Dense & $h$      & SELU \\
% Output   & Dense & $m$      & Identity \\
% \hline
% \end{tabular}
% \end{minipage}
% \end{table}


\paragraph{Training Details}
Models are implemented in \texttt{Keras}, which provides a convenient interface for specification and training. Optimization uses the \textbf{ADAM} algorithm with default parameters:
\[
\beta_1 = 0.9,\ \beta_2 = 0.999,\ \epsilon = 10^{-7}.
\]

Model training was accomplished using Quantile Loss, see in \cref{methods:scores}.

To prevent overfitting and reduce training time, we apply early stopping based on validation loss: training halts if no improvement occurs for 10 consecutive epochs.

\subsection{Distribution Function - Linear Interpolation}

The output from the neural nets consists of estimated quantiles, which need to be converted to into a full continuos distribution for the further steps of the process.

Here the decision was made to use a standard piecewise linear interpolation.

\begin{equation}
\hat{F}(x) = p(x|P,Q) = 
\begin{cases}
0, & x < q_0, \\
q_i + \frac{x - q_i}{q_{i+1} - q_i} (p_{i+1} - p_i) , & q_i \le x < q_{i+1}, \\
1, & x \ge q_{m+1},
\end{cases}
\end{equation}

Where $p_i$ is the cumulative probability corresponding to the estimated value quantile value $q_i$

A distribution function needs to be monotone and increasing,  linear interpolation preserves monotonicity assuming the interpolated points are increasing.

The simplicity of the method also means low computational complexity and quick calculations, desirable properties in many situations.

Other methods were also considered, higher order interpolation would provide other desirable properties. e.g. higher order convergence or continuous first derivatives. However these higher order models requires addition constraints to ensure monotonicity, as well as having increased computational complexity. 


\subsection{Correlation Model - SARIMA}

To capture the temporal correlation structure in the time series, we employ a Seasonal AutoRegressive Integrated Moving Average (SARIMA) model. SARIMA extends the classical ARIMA model by incorporating seasonality, making it suitable for data with repeating seasonal patterns.

ARIMA models assume that the current observation is a linear combination of past observations (Auto-Regressive part) and past forecast errors (Moving Average part), after differencing to achieve stationarity. SARIMA extends this framework by adding seasonal autoregressive and moving average components, which repeat at a specified seasonal lag, allowing the model to capture periodic patterns in the data.

The general SARIMA formulation is:
\begin{equation}
    \phi(B)\Phi(B^s)(1 - B)^d(1 - B^s)^D y_t = \theta(B)\Theta(B^s) \epsilon_t,
\end{equation}

where:
\begin{align*}
    & \phi(B) = 1 - \phi_1 B - \dots - \phi_p B^p, \\
    & \theta(B) = 1 + \theta_1 B + \dots + \theta_q B^q, \\
    & \Phi(B^s), \Theta(B^s) \text{ are seasonal AR and MA polynomials}, \\
    & B^k y_t = y_{t-k} \text{ (lag operator)}.
\end{align*}

The primary reasons for choosing SARIMA models are their simplicity, flexibility. SARIMA models are computationally not heavy, being both easy to estimate and evaluate. In addition the models are well-studied in general easy to interpret. While simple, the models are capable of expressing a variety of correlation structures. 

SARIMA models assume each observation is expressed as sum of independent normal variables. Therefore the marginal distributions for SARIMA models are also normal. As  we transform observations into pseudo-residual space, the marginal distribution of the transformed data is ideally normal. SARIMA model therefore seem like a natural choice for modelling the correlation structure.

\subsection{Scores}
\label{methods:scores}

To evaluate the performance of the proposed method, we use four commonly applied metrics: Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Continuous Ranked Probability Score (CRPS), and Variogram Score (VarS). These scores capture different aspects of forecast quality.

\begin{align}
    RMSE &= \sqrt{\frac{1}{N}\sum_{i=1}^{N} (y_i - \hat{y}_i)^2}, \\
    MAE  &= \frac{1}{N}\sum_{i=1}^{N} |y_i - \hat{y}_i|, \\
    CRPS &= \int_{-\infty}^\infty (F(u) - \mathbb{I}_{y<u})^2 \, du 
          = \mathbb{E}[|X - y|] - \tfrac{1}{2}\mathbb{E}[|X - X'|], \\
    VarS_p &= \sum_{i=1}^k \sum_{j=i+1}^k w_{i,j}\Big(|y_i - y_j|^p - \mathbb{E}[|X_{t_i} - X_{t_j}|^p]\Big)^2.
\end{align}

For VarS, we use equal weights $w_{i,j} = \frac{1}{k(k-1)}$, though inverse distance weighting $w_{i,j} = \frac{1}{|i-j|}$ is also common.

As an additional metric, we use the quantile score (QS) as the loss function for training the neural networks:

\begin{equation}
    QS_\alpha(y,x) = 
    \begin{cases}
        \alpha(y - x), & y \ge x, \\
        (1-\alpha)(x - y), & y < x.
    \end{cases}
\end{equation}

This scores is very related to the MAE, in that MAE is equivalent to QS with $\alpha = 0.5$, and also the CRPS, as the CRPS is the mean QS over all quantiles.

\subsubsection{Evaluating Scores}

RMSE and MAE are straightforward to compute, while CRPS and VarS require expectations over the predictive distribution. Since the model is highly non-linear and simulation-based, these expectations are approximated via Monte Carlo simulation.

To a evaluate the scores we We generate $n$ simulated paths with horizon $k$ from the correlation model and transform them back to the original space. Let $\hat{Y}_{t,i} = [\hat{y}_{t,i}, \hat{y}_{t+1,i}, \dots, \hat{y}_{t+k,i}]$ denote simulation $i$. Then:

\begin{align}
    \mathbb{E}[|X - y|] &\approx \frac{1}{n}\sum_{i=1}^n |\hat{y}_{t,i} - y|, \\
    \mathbb{E}[|X - X'|] &\approx \frac{1}{n(n-1)}\sum_{i=1}^n \sum_{j=i+1}^n |\hat{y}_{t,i} - \hat{y}_{t,j}|, \\
    \mathbb{E}[|X_{t_i} - X_{t_j}|^p] &\approx \frac{1}{n}\sum_{z=1}^n |\hat{y}_{t_i,z} - \hat{y}_{t_j,z}|^p.
\end{align}