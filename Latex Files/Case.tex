\section{Case Study}
\label{case}

The primary motivation for this work is to improve probabilistic forecasts of wind power production, following the framework of \citet{jorgensenSequentialMethodsError2025}. We here use the same same data for a case study. case study,

\subsection{Data}
\label{case-data}

The dataset consists of observed wind-power production and an ensemble of physically based production forecasts for the two Danish electricity bidding zones, DK1 and DK2, each further divided into onshore and offshore wind production.
The two zones differ substantially in geography, installed capacity, and interconnection structure:

\begin{itemize}
    \item DK1 is interconnected with continental Europe.
    \item DK2 is interconnected with the Nordic countries.
\end{itemize}

Each zone exhibits distinct production dynamics, but the time series show clear cross-zone correlation.
Notably, DK1 onshore has by far the largest installed capacity—approximately equal to the other three subzones combined.

A representative example of the full dataset is shown in \cref{caseproduction}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{Results/Graphs/data_example.pdf}
    \caption{Caption}
    \label{fig:casedata}
\end{figure}

\subsubsection{Observed power Production}
\label{case-data-observed}
Observed production data spans the period
2021‑12‑31 23:00:00 to 2024‑10‑12 22:00:00 (UTC).
Data was obstained from the Danish TSO Energinet, via their open data service
\href{https://www.energidataservice.dk/}{Energi Data Service} \cite{energinetEnergiDataService}.

Energi Data Service provides two similar datasets for energy production, a real-time data set given in 5-minute intervals. and a settlement dataset for statistical  porpuses.

The real-time data set is provided as soon as available, values are based on scaled measurements, thus uncertain, and data errors occur. \cite{energinet_electricityprodex5min_2020}

The statistical dataset is report at hourly intervals and based on settlement data. This dataset is more reliable, but only available at a 9-15 day delay.\cite{energinet_production_consumption_settlement_2020}

For this case study it was chosen to use data set for statistical purposes.

A statistical summary of the observed production for all four zones is provided in \cref{tab:data:observation}.

\input{Results/Tables/observations_summary}

\subsubsection{Ensemble forecast}
The other part of the case data is a proprietary ensemble forecast derived from the ECMWF's (European center for midrange weather forecasts) weather forecasts. And consists of:

\begin{itemize}
    \item 1 high‑probability ensemble (HPE)
    \item 50 perturbed ensemble members, generated by perturbing the initial conditions of the HPE.
\end{itemize}

 The ECMWF Forecast is provided at hourly resolution, With the forecast for the day ahead, being made at 12:00 the day before, and several hours after. \cite{ecmwf_about_forecasts}

The individual ensemble members are converted into production forecasts using a proprietary method. Due to the proprietary nature of the method, the ensemble data cannot be disclosed. In iaddition to this case study we also make a simulation study described in \cref{simulation}

\subsubsection{Data anomelies}

Some problematic effects exists in the data.

First is the problem of curtailment. At certain times, producers are paid to reduce generation due to market or transmission constraints, leading to artificially low production values \cite{olson2014a}. In the prior work by \citet{jorgensenSequentialMethodsError2025} These periods of curtailment were remove from the data, as they are not representative of the normal system behaviour.

In addition the ensemble forecast has clear problems in the zone DK2-onshore. In this zone the ensembles occasional collapse to a value $\approx  315 MWh$. These periods where likewise removed in the paper by \citet{jorgensenSequentialMethodsError2025}.

For this case study we keep these periods in. Removing the periods would introduce further problems of missing data, or discontinuous time intervals. 

\subsection{Data Cleaning}

Only light preprocessing of data was necessary:

\begin{itemize}
    \item Zero‑valued observations: Observations equal to exactly zero lead to numerical instability during the probability‑integral-transform stage of the pipeline. To avoid this, values of 0 were replaced with a small positive constant (0.01), which is practically indistinguishable at operational scales.
    \item Timestamp harmonization: the observational and ensemble datasets used slightly different time formats. Both were converted to UTC to ensure alignment
\end{itemize} 

No additional filtering or smoothing was applied.

\subsection{Data Split}

We adopt a straightforward temporal train–test split, ensuring both train- and test set was non-overlapping and uninterrupted intervals. \cite[cp. 3]{theodoridis2020a}

The split was chosen such that there was one year of data in the test set, allowing evaluation across all seasons as wind‑power production exhibits strong seasonal variability\cite{wan2012a}.

The split Was chosen as 53.3\%-46.7\% train-test with the data split at 2023-02-23-22 giving 9983 time points ($\approx$ 416 days) in the train set and 8760 time points ($\approx$ 365 days) in the test set

\subsection{Procedure}
Each zone is modeled independently, meaning:

\begin{itemize}
    \item one marginal quantile model per zone
    \item one correlation (SARIMA) model per zone
    \item no cross‑zone information is used
\end{itemize}

The marginal models use the ensemble forecast at the current time and up to lag 48 hours as covariates:

\[
X_t = [\mathbf{x}_t, x_{t-1},\dots, x_{t-48}]
\]

where  $\mathbf{x}_t \in\mathbb{R}^{51} $ are the 51 individual ensemble members at time t.

Only ensemble information is used as covariates; no observational data enters the forecast at prediction time. This mirrors operational conditions for ensemble‑based forecasting.

To ensure our correlation corrected forecast is made without additional information the stationary distribution of the SARIMA model should be used. due to implementational constraints, we instead forecasted a long "burn-in" period (h = 10000) was simulated, only data points after the burn-in period.

For evaluating scores k = 1000 simulations was made from each model, in normal space. To compare performace scores for the pure marginal models were also calculated. For these models random iid. sample were drawn from a standard normal distribution then converted back into original space.

For comparison the scores for the ensembles were also calculated, This was done using only the 51 ensembles, The result of which is therefore more uncertain.

\subsection{Results}

An example of the forecast provided by the method can be seen in \cref{fig:forecastexample}. Where the correlation corrected forecast is provided. The period visualised is the same as in \cref{fig:casedata} for comparison purpose.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Results/Graphs/corrolation_example.pdf}
    \caption{Caption}
    \label{fig:forecastexample}
\end{figure}

The loss curves during the training of the neural nets can be seen in \cref{fig:loss}. Showing both the training and validation loss.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Results/Graphs/loss_curves.pdf}
    \caption{Caption}
    \label{fig:loss}
\end{figure}

To inform SARIMA model structure. The ACF and PACf for the pseudoresiduals are plotted in \cref{fig:acf_pacf} both before an after applying the correlation models.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Results/Graphs/acf_pacf.pdf}
    \caption{Caption}
    \label{fig:acf_pacf}
\end{figure}


For the SARIMA models the estimated parameters are found in \cref{tab:parameters}. Where parameters estimated using both the simple- and latent model are presented.

\input{Results/Tables/parameters}

The finally performance metrics for all zones and models are presented in \cref{tab:scores}. showing the raw ensemble scores, as well as model scores as a percentage of the ensemble scores.

\input{Results/Tables/scores}